<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>13-tensorflow on 工具书-mlzhang</title><link>http://lanms.github.io/docs/13-tensorflow/</link><description>Recent content in 13-tensorflow on 工具书-mlzhang</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://lanms.github.io/docs/13-tensorflow/index.xml" rel="self" type="application/rss+xml"/><item><title>01-tensorflow环境搭建</title><link>http://lanms.github.io/docs/13-tensorflow/01-tensorflow%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://lanms.github.io/docs/13-tensorflow/01-tensorflow%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid><description>01-tensorflow环境搭建 # tf 的主要依赖 # 1. Protocol Buffer # 一款谷歌开发的处理结构化数据的工具，处了Protocol Buffer 外还有 XML / JSON等结构化数据处理工具 其序列化后的数据是二进制流，需要先定义数据的格式（schema） 数据占用空间小，解析时间快 文件格式 .proto，每个 message 代表了一类结构化的数据 message user{ optional string name = 1; required int32 id = 2; repeated string email = 3;}如上：message 里定义了每一个属性的类型和名字。
Bazel # 谷歌开源的自动化构建工具，谷歌内部绝大部分的应用都是通过它来编译的 安装
pip install tensorflow
建议使用 anaconda
conda install tensorflow</description></item><item><title>02-tf基础知识和模拟线性回归</title><link>http://lanms.github.io/docs/13-tensorflow/02-tf%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%92%8C%E6%A8%A1%E6%8B%9F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://lanms.github.io/docs/13-tensorflow/02-tf%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%92%8C%E6%A8%A1%E6%8B%9F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid><description>TensorFlow # Tensor（张量）， 意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow即为张量从图的一端流动到另一端
支持 CNN（卷积神经网络）、RNN（循环神经网络）和 LSTM（长短期记忆网络）算法，
是目前在 Image,NLP最流行的深度学习神经网络模型
对比传统深度学习，为什么使用TensorFlow # 深度学习意味着建立具有很多层的大规模神经网络 除了输入 X，函数还使用一系列的参数，其中包括标量值、向量以及最昂贵的矩阵和高级张量 在训练网络之前，需要定义一个代价函数，常见的代价函数包括回归问题的方差以及分类时候的交叉熵 训练时，需要连续将多批新输入投入网络，对所有的参数求导后，代入代价函数，从而更新整个网络模型 这个过程中有两个主要问题： 较大的数字或者张量在一起相乘百万次的处理使整个模型代价非常大 手动求导耗时非常久 所以，TensorFlow 的对函数的自动求导和分布式计算，可以帮助我们节省很多时间。
TensorFlow优点 # 基于Python，写的很快并且具有高可读性 在多GPU系统上运行更为顺畅 代码编译效率较高 社区发展非常迅速并且很活跃 能够生成显示网络拓扑结构和性能的可视化图 原理 # TensorFlow是用数据流图（data flow graphs）技术来进行数值计算的 数据流图是描述有向图中数值计算过程 有向图中，节点通常代表数学运算，边表示节点之间的某种联系，它负责传输多为数据（Tensors） 节点可以被分配到多个计算设备上，可以异步合并地执行操作。因为是有向图，所以只有等到之前的节点们的计算状态完成后，当前节点才能执行操作 使用 # 使用（graph）来表示任务 在被称为会话（Session）的上下文（context）中执行图 使用 tensor 表示数据 通过变量（Variable） 维护状态 使用 feed 和 fetch 可以为任意操作（arbitray operation）赋值或者从中获取数据 实例：hello TensorFlow</description></item><item><title>03-基础指令</title><link>http://lanms.github.io/docs/13-tensorflow/03-%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://lanms.github.io/docs/13-tensorflow/03-%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4/</guid><description>constant # 定义常量 a = tf.constant([1.0, 2.0], name=&amp;#34;a&amp;#34;) 关于张量 # 在 TensorFlow中，所有的数据都是通过张量的形式来表示。但是张量在 TensorFlow 中实现并不是直接采用数组的形式，它是对 TF 中运算结果的引用。在张量中并没有真正的保存数据，它保存的是如何得到数据的计算过程。
一个张量中主要保存了 name shape type 的三种属性
前向传播算法 #</description></item></channel></rss>